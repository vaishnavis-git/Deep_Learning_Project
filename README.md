# Enhancing Text Style Transfer for AI-Generated Posters

## Project Overview
Text-to-image generation models have improved significantly since OpenAI's DALL-E was announced in January, 2021. In 2022, the output of state of the art text-to-image models, such as OpenAI's DALL-E 2, Google Brain's Imagen, StabilityAI's Stable Diffusion, and Midjourney began to approach good quality images. These models have made great improvements on the quality of the images and stylistic aspects. Earlier, the text spellings generated by these models on logos, posters etc. used to be very inaccurate, with either missing, incorrect or unreadable characters on images but there has been significant improvement recently. For instance, just in March, 2024, Stable Diffusion published their paper on Stable Diffusion 3 which uses a novel Multimodal Diffusion Transformer (MMDiT) architecture, vastly improving text understanding and spelling capabilities compared to all previously existing models.

UDiffText a model that corrects text on images by incorporating a character-level encoder and utilizes a pre-trained diffusion model, is also robust in addressing incorrect spellings. Although it outperforms all SOTA models in correcting text, it does not render the rest of the poster well.

## Our Objectives
**Enhancing the style of the font on the masked text region**
**Maintaining the overall style of the poster**
